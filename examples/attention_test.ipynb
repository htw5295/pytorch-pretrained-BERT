{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport IPython.display as display\n\nimport argparse\nimport collections\nimport logging\nimport json\nimport re\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nlogging.basicConfig(format \u003d \u0027%(asctime)s - %(levelname)s - %(name)s -   %(message)s\u0027,\n                    datefmt \u003d \u0027%m/%d/%Y %H:%M:%S\u0027,\n                    level \u003d logging.INFO)\nlogger \u003d logging.getLogger(__name__)\n\nvis_html \u003d \"\"\"\n  \u003cspan style\u003d\"user-select:none\"\u003e\n    Layer: \u003cselect id\u003d\"layer\"\u003e\u003c/select\u003e\n    Attention: \u003cselect id\u003d\"att_type\"\u003e\n      \u003coption value\u003d\"all\"\u003eAll\u003c/option\u003e\n      \u003coption value\u003d\"a\"\u003eSentence A self-attention\u003c/option\u003e\n      \u003coption value\u003d\"b\"\u003eSentence B self-attention\u003c/option\u003e\n      \u003coption value\u003d\"ab\"\u003eSentence A -\u003e Sentence B\u003c/option\u003e\n      \u003coption value\u003d\"ba\"\u003eSentence B -\u003e Sentence A\u003c/option\u003e\n    \u003c/select\u003e\n  \u003c/span\u003e\n  \u003cdiv id\u003d\u0027vis\u0027\u003e\u003c/div\u003e\n\"\"\"\n\n__location__ \u003d os.path.realpath(\n    os.path.join(os.getcwd(), os.path.dirname(__file__)))\nvis_js \u003d open(os.path.join(__location__, \u0027attention.js\u0027)).read()\n\n\nclass InputExample(object):\n\n    def __init__(self, unique_id, text_a, text_b):\n        self.unique_id \u003d unique_id\n        self.text_a \u003d text_a\n        self.text_b \u003d text_b\n\n\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, unique_id, tokens, token_a, token_b, input_ids, input_mask, input_type_ids):\n        self.unique_id \u003d unique_id\n        self.tokens \u003d tokens\n        self.token_a \u003d token_a\n        self.token_b \u003d token_b\n        self.input_ids \u003d input_ids\n        self.input_mask \u003d input_mask\n        self.input_type_ids \u003d input_type_ids\n\n\ndef convert_examples_to_features(examples, seq_length, tokenizer):\n    \"\"\"Loads a data file into a list of `InputFeature`s.\"\"\"\n\n    features \u003d []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a \u003d tokenizer.tokenize(example.text_a)\n\n        tokens_b \u003d None\n        if example.text_b:\n            tokens_b \u003d tokenizer.tokenize(example.text_b)\n\n        if tokens_b:\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n            _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with \"- 2\"\n            if len(tokens_a) \u003e seq_length - 2:\n                tokens_a \u003d tokens_a[0:(seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where \"type_ids\" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type\u003d0` and\n        # `type\u003d1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambigiously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the \"sentence vector\". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens \u003d []\n        token_a \u003d []\n        token_b \u003d []\n        input_type_ids \u003d []\n        token_a.append(\"[CLS]\")\n        input_type_ids.append(0)\n        for token in tokens_a:\n            token_a.append(token)\n            input_type_ids.append(0)\n        token_a.append(\"[SEP]\")\n        input_type_ids.append(0)\n\n        if tokens_b:\n            for token in tokens_b:\n                token_b.append(token)\n                input_type_ids.append(1)\n            token_b.append(\"[SEP]\")\n            input_type_ids.append(1)\n\n        tokens \u003d token_a + token_b\n        input_ids \u003d tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask \u003d [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) \u003c seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            input_type_ids.append(0)\n\n        assert len(input_ids) \u003d\u003d seq_length\n        assert len(input_mask) \u003d\u003d seq_length\n        assert len(input_type_ids) \u003d\u003d seq_length\n\n        if ex_index \u003c 5:\n            logger.info(\"*** Example ***\")\n            logger.info(\"unique_id: %s\" % (example.unique_id))\n            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n            logger.info(\n                \"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n\n        features.append(\n            InputFeatures(\n                unique_id\u003dexample.unique_id,\n                tokens\u003dtokens,\n                token_a\u003dtoken_a,\n                token_b\u003dtoken_b,\n                input_ids\u003dinput_ids,\n                input_mask\u003dinput_mask,\n                input_type_ids\u003dinput_type_ids))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\u0027s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length \u003d len(tokens_a) + len(tokens_b)\n        if total_length \u003c\u003d max_length:\n            break\n        if len(tokens_a) \u003e len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef read_examples(input_file):\n    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n    examples \u003d []\n    unique_id \u003d 0\n    with open(input_file, \"r\", encoding\u003d\u0027utf-8\u0027) as reader:\n        while True:\n            line \u003d reader.readline()\n            if not line:\n                break\n            line \u003d line.strip()\n            text_a \u003d None\n            text_b \u003d None\n            m \u003d re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n            if m is None:\n                text_a \u003d line\n            else:\n                text_a \u003d m.group(1)\n                text_b \u003d m.group(2)\n            examples.append(\n                InputExample(unique_id\u003dunique_id, text_a\u003dtext_a, text_b\u003dtext_b))\n            unique_id +\u003d 1\n    return examples\n\n\ndef get_attentions(tokens_a, tokens_b, attn):\n    \"\"\"Compute representation of the attention to pass to the d3 visualization\n    Args:\n      tokens_a: tokens in sentence A\n      tokens_b: tokens in sentence B\n      attn: numpy array, attention\n          [num_layers, batch_size, num_heads, seq_len, seq_len]\n    Returns:\n      Dictionary of attention representations with the structure:\n      {\n        \u0027all\u0027: Representations for showing all attentions at the same time. (source \u003d AB, target \u003d AB)\n        \u0027a\u0027: Sentence A self-attention (source \u003d A, target \u003d A)\n        \u0027b\u0027: Sentence B self-attention (source \u003d B, target \u003d B)\n        \u0027ab\u0027: Sentence A -\u003e Sentence B attention (source \u003d A, target \u003d B)\n        \u0027ba\u0027: Sentence B -\u003e Sentence A attention (source \u003d B, target \u003d A)\n      }\n      and each sub-dictionary has structure:\n      {\n        \u0027att\u0027: list of inter attentions matrices, one for each layer. Each is of shape [num_heads, source_seq_len, target_seq_len]\n        \u0027top_text\u0027: list of source tokens, to be displayed on the left of the vis\n        \u0027bot_text\u0027: list of target tokens, to be displayed on the right of the vis\n      }\n    \"\"\"\n\n    all_attns \u003d []\n    a_attns \u003d []\n    b_attns \u003d []\n    ab_attns \u003d []\n    ba_attns \u003d []\n    slice_a \u003d slice(0, len(tokens_a)) # Positions corresponding to sentence A in input\n    slice_b \u003d slice(len(tokens_a), len(tokens_a) + len(tokens_b)) # Position corresponding to sentence B in input\n    num_layers \u003d len(attn)\n    for layer in range(num_layers):\n        layer_attn \u003d attn[layer][0] # Get layer attention (assume batch size \u003d 1), shape \u003d [num_heads, seq_len, seq_len]\n        all_attns.append([[[round(float(k), 3) for k in j] for j in i] for i in layer_attn.tolist()]) # Append AB-\u003eAB attention for layer, across all heads\n        a_attns.append([[[round(float(k), 3) for k in j] for j in i] for i in layer_attn[:, slice_a, slice_a].tolist()]) # Append A-\u003eA attention for layer, across all heads\n        b_attns.append([[[round(float(k), 3) for k in j] for j in i] for i in layer_attn[:, slice_b, slice_b].tolist()]) # Append B-\u003eB attention for layer, across all heads\n        ab_attns.append([[[round(float(k), 3) for k in j] for j in i] for i in layer_attn[:, slice_a, slice_b].tolist()]) # Append A-\u003eB attention for layer, across all heads\n        ba_attns.append([[[round(float(k), 3) for k in j] for j in i] for i in layer_attn[:, slice_b, slice_a].tolist()]) # Append B-\u003eA attention for layer, across all heads\n\n    attentions \u003d  {\n        \u0027all\u0027: {\n            \u0027att\u0027: all_attns,\n            \u0027top_text\u0027: tokens_a + tokens_b,\n            \u0027bot_text\u0027: tokens_a + tokens_b\n        },\n        \u0027a\u0027: {\n            \u0027att\u0027: a_attns,\n            \u0027top_text\u0027: tokens_a,\n            \u0027bot_text\u0027: tokens_a\n        },\n        \u0027b\u0027: {\n            \u0027att\u0027: b_attns,\n            \u0027top_text\u0027: tokens_b,\n            \u0027bot_text\u0027: tokens_b\n        },\n        \u0027ab\u0027: {\n            \u0027att\u0027: ab_attns,\n            \u0027top_text\u0027: tokens_a,\n            \u0027bot_text\u0027: tokens_b\n        },\n        \u0027ba\u0027: {\n            \u0027att\u0027: ba_attns,\n            \u0027top_text\u0027: tokens_b,\n            \u0027bot_text\u0027: tokens_a\n        }\n    }\n\n    return attentions\n\n\ndef main():\n    parser \u003d argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(\"--input_file\", default\u003d\"./examples/test.txt\", type\u003dstr)\n    parser.add_argument(\"--output_file\", default\u003d\"./examples/output.txt\", type\u003dstr)\n\n    ## Other parameters\n    parser.add_argument(\"--layers\", default\u003d\"-1,-2,-3,-4\", type\u003dstr)\n    parser.add_argument(\"--max_seq_length\", default\u003d128, type\u003dint,\n                        help\u003d\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n                            \"than this will be truncated, and sequences shorter than this will be padded.\")\n    parser.add_argument(\"--batch_size\", default\u003d32, type\u003dint, help\u003d\"Batch size for predictions.\")\n    parser.add_argument(\"--local_rank\",\n                        type\u003dint,\n                        default\u003d-1,\n                        help \u003d \"local_rank for distributed training on gpus\")\n    parser.add_argument(\"--cuda\",\n                        default\u003dFalse,\n                        action\u003d\u0027store_true\u0027,\n                        help\u003d\"Whether not to use CUDA when available\")\n\n    args \u003d parser.parse_args()\n\n    if args.local_rank \u003d\u003d -1 or not args.cuda:\n        device \u003d torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        n_gpu \u003d torch.cuda.device_count()\n    else:\n        device \u003d torch.device(\"cuda\", args.local_rank)\n        n_gpu \u003d 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend\u003d\u0027nccl\u0027)\n    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank !\u003d -1)))\n\n    layer_indexes \u003d [int(x) for x in args.layers.split(\",\")]\n\n    tokenizer \u003d BertTokenizer.from_pretrained(\u0027./examples/multi_cased_L-12_H-768_A-12\u0027, do_lower_case\u003dFalse)\n\n    examples \u003d read_examples(args.input_file)\n\n    features \u003d convert_examples_to_features(\n        examples\u003dexamples, seq_length\u003dargs.max_seq_length, tokenizer\u003dtokenizer)\n\n    unique_id_to_feature \u003d {}\n    for feature in features:\n        unique_id_to_feature[feature.unique_id] \u003d feature\n\n    model \u003d BertModel.from_pretrained(\u0027./examples/multi_cased_L-12_H-768_A-12\u0027)\n    model.to(device)\n\n    if args.local_rank !\u003d -1:\n        model \u003d torch.nn.parallel.DistributedDataParallel(model, device_ids\u003d[args.local_rank],\n                                                          output_device\u003dargs.local_rank)\n    elif n_gpu \u003e 1:\n        model \u003d torch.nn.DataParallel(model)\n\n    all_input_ids \u003d torch.tensor([f.input_ids for f in features], dtype\u003dtorch.long)\n    all_input_type_ids \u003d torch.tensor([f.input_type_ids for f in features], dtype\u003dtorch.long)\n    all_input_mask \u003d torch.tensor([f.input_mask for f in features], dtype\u003dtorch.long)\n    all_token_a \u003d [f.token_a for f in features]\n    all_token_b \u003d [f.token_b for f in features]\n    all_example_index \u003d torch.arange(all_input_ids.size(0), dtype\u003dtorch.long)\n\n    eval_data \u003d TensorDataset(all_input_ids, all_input_type_ids, all_input_mask, all_example_index)\n    if args.local_rank \u003d\u003d -1:\n        eval_sampler \u003d SequentialSampler(eval_data)\n    else:\n        eval_sampler \u003d DistributedSampler(eval_data)\n    eval_dataloader \u003d DataLoader(eval_data, sampler\u003deval_sampler, batch_size\u003dargs.batch_size)\n\n    model.eval()\n    i \u003d 0\n    with open(args.output_file, \"w\", encoding\u003d\u0027utf-8\u0027) as writer:\n        for input_ids, input_type_ids, input_mask, example_indices in eval_dataloader:\n            input_ids \u003d input_ids.to(device)\n            input_type_ids \u003d input_type_ids.to(device)\n            input_mask \u003d input_mask.to(device)\n\n            # attn \u003d attention_visualizer.get_attention(input_ids, input_type_ids, input_mask)\n            # print(attn)\n            all_encoder_layers, _, attn \u003d model(input_ids, token_type_ids\u003dinput_type_ids, attention_mask\u003dinput_mask)\n            attn_tensor \u003d torch.stack([attn_data[\u0027attn_probs\u0027] for attn_data in attn])\n            attn \u003d attn_tensor.data.numpy()\n            attentions \u003d get_attentions(all_token_a[i], all_token_b[i], attn)\n            att_json \u003d json.dumps(attentions)\n            display.display(display.HTML(vis_html))\n            display.display(display.Javascript(\u0027window.attention \u003d %s\u0027 % att_json))\n            display.display(display.Javascript(vis_js))\n            i +\u003d 1\n            # all_encoder_layers \u003d all_encoder_layers\n            #\n            # for b, example_index in enumerate(example_indices):\n            #     feature \u003d features[example_index.item()]\n            #     unique_id \u003d int(feature.unique_id)\n            #     # feature \u003d unique_id_to_feature[unique_id]\n            #     output_json \u003d collections.OrderedDict()\n            #     output_json[\"linex_index\"] \u003d unique_id\n            #     all_out_features \u003d []\n            #     for (i, token) in enumerate(feature.tokens):\n            #         all_layers \u003d []\n            #         for (j, layer_index) in enumerate(layer_indexes):\n            #             layer_output \u003d all_encoder_layers[int(layer_index)].detach().cpu().numpy()\n            #             layer_output \u003d layer_output[b]\n            #             layers \u003d collections.OrderedDict()\n            #             layers[\"index\"] \u003d layer_index\n            #             layers[\"values\"] \u003d [\n            #                 round(x.item(), 6) for x in layer_output[i]\n            #             ]\n            #             all_layers.append(layers)\n            #         out_features \u003d collections.OrderedDict()\n            #         out_features[\"token\"] \u003d token\n            #         out_features[\"layers\"] \u003d all_layers\n            #         all_out_features.append(out_features)\n            #     output_json[\"features\"] \u003d all_out_features\n            #     writer.write(json.dumps(output_json) + \"\\n\")\n\n\nif __name__ \u003d\u003d \"__main__\":\n    main()\n"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}